%This is the text for chapter 2 of the report 

<<set-parent, echo=FALSE, cache=FALSE>>=
set_parent('report.Rnw')
@

\chapter{Specification of ERGM Count}
In this chapter, we will first define ERGM Count and its relevant shaping functions as proposed by \citet{countergmdefined}. 
Then, we will introduce the simplest form of ERGM Count which are modeled Poisson and Geometric Distribution. 
The various shapes of ERGM Count will then be explored and the interpretation of each the shapes will also be discussed accordingly. 

\section{Model Definition}
Let the set of actors in the network be $N$ and $n$ represent the number of actors involved, such that $n \equiv |N|$.
Let $\mathbb{Y}$ be the set of all dyads where a dyad represent a unique pair of actors that may have relational interest. 
Since the later applied model is directed, $\mathbb{Y} \subseteq N \times N$. 

Let the edge $y_{ij}$ represents the value of relational interest of dyad $(i,j)$ that originates from actor $i$ and received by actor $j$.
In Binary ERGM, each edge can only have binary values, $y_{ij} \in \{0,1\}$. 
But in ERGM Count, an edge can take the value of any whole numbers, such that $y_{ij} \in \mathbb{N}_0$. 
Hence the set of possible network configuration, or the sample space is a set $\mathcal{Y} \subseteq \mathbb{N}_{0}^{\mathbb{Y}}$. 
Let the random network of count configuration be $\bm{Y} \in \mathcal{Y}$, with $\bm{y}$ as its realization.

The unknown model parameter $\bm{\theta} \in \Theta$ where $\Theta \subseteq \mathbb{R}^{q}$ is then mapped to $\mathbb{R}^p$ by the function $\eta:\Theta \to \mathbb{R}^p$. Given chosen sufficient statistics $g:\mathcal{Y} \to \mathbb{R}^{p}$ (which may also depend on external covariate $\bm{x}$), and reference measure $h:\mathcal{Y} \to [0,\infty)$. The ERGM Count of $\bm{Y}$ has Probability Mass Function (PMF):
\begin{align}
Pr_{\bm{\theta};h,\eta,g}(\bm{Y}=\bm{y})=\frac{h(\bm{y})\exp(\eta(\bm{\theta})\cdot g(\bm{y}))}{k_{h,\eta,g}(\bm{\theta})},
\end{align}
where the denominator is a normalization constant defined as:
\begin{align}
k_{h,\eta,g}(\bm{\theta}) = \sum_{\bm{y} \in \mathcal{Y}}h(\bm{y})\exp(\eta(\bm{\theta})\cdot g(\bm{y})).
\end{align}
There is an additional constraint for model parameter $\bm{\theta}$ to be an element from the set:
\begin{align} \label{eq: parameter constraint}
\Theta \subseteq \Theta_{N}=\{\theta' \in \mathbb{R}^q: k_{h,\eta,g}(\bm{\theta}) < \infty\}
\end{align}
This constraint is not necessary in Binary ERGM as their sample space is finite \citep{countergmdefined}. 
In many occasions, the constraint for the parameters is not known explicitly. 

To simplify the notation, the PMF can be defined up to normalization constant as:
\begin{align}
Pr(\bm{Y}=\bm{y}|\bm{\theta}) \propto h(\bm{y})\exp(\eta(\bm{\theta})\cdot g(\bm{y}))
\end{align}
This assumes the constraint \ref{eq: parameter constraint} is met.

\section{Reference Measure and Baseline Distribution}
In Binary ERGM, if the model has \textit{dyadic independence} property, the model will reduce to edge-wise logistic regression. 
\citet{countergmdefined} stated that ERGM of valued network should reduce into Generalized Linear Model given \textit{dyadic independence}.

The specification of an appropriate reference measure enable such relationship to occur. 
Suppose the model only has one predictor $\theta$, with sufficient statistics, \BI{sum} : $g(\bm{y}) = \sum_{(i,j) \in \mathbb{Y}} y_{ij}$. 
If the reference measure, $h(\bm{y}) = 1$, the model's PMF will be:
\begin{align}\label{eq: edgewise Geometric}
Pr(\bm{Y}=\bm{y}|\theta) \propto \exp\left(\theta \sum_{(i,j) \in \mathbb{Y}} y_{ij}\right) = \prod_{(i,j) \in \mathbb{Y}} (\exp(\theta))^{y_{ij}}
\end{align} 
We recognize this as an edge-wise distribution $Y_{ij}\stackrel{\text{iid}}{\sim}\text{Geometric}(p =1-\exp(\theta))$ where $\theta < 0$. This corresponds to the baseline distribution of \textit{geometric-reference}. 
Whereas  if the reference measure is $h(\bm{y}) = \prod_{(i,j) \in \mathbb{Y}}(y_{ij}!)^{-1}$, the model will have PMF:
\begin{align}\label{eq: edgewise Poisson}
Pr(\bm{Y}=\bm{y}|\theta) \propto \frac{\exp\left(\theta \sum_{(i,j) \in \mathbb{Y}} y_{ij}\right)}{\prod_{(i,j) \in \mathbb{Y}}y_{ij}!} = \prod_{(i,j) \in \mathbb{Y}} \frac{(\exp(\theta))^{y_{ij}}}{y_{ij}!}
\end{align}
We recognize this as another edge-wise distribution $Y_{ij}\stackrel{\text{iid}}{\sim}\text{Poisson}(\lambda =\exp(\theta))$ where $\theta \in \mathbb{R}$. 
This corresponds to the baseline distribution of \textit{poisson-reference}. 
Notice that the 2 models have different model parameter constraints. 
We illustrate the shape of Poisson and Geometric Distribution in Figure \ref{fig: Geometric and Poisson} below.

\begin{figure}[H] 
<<geom_pois,fig.asp=0.45>>=
geom_pois <- readRDS(paste(root,"/plots/geom_pois",sep=""))
plot(geom_pois)
@
\caption [Geometric and Poisson Distribution with Mean = 3.5]{These plots show the shape of Geometric and Poisson Distribution. Both distributions have equal mean, $\mu = \frac{3}{2}$. Differs from Poisson, Geometric Distribution has a strictly decreasing probability as $y_{ij}$ increased.}
\label{fig: Geometric and Poisson}
\end{figure}

On a side note, there is an error in \citet{countergmdefined} on the normalization constant of the 2 distributions above. 
For equation \ref{eq: edgewise Geometric}, \citet{countergmdefined} normalization constant is $\prod_{(i,j) \in \mathbb{Y}}\frac{1}{1-\exp(\theta)}$ when it is supposed to be  $\prod_{(i,j) \in \mathbb{Y}}1-\exp(\theta)$.
Whereas for equation \ref{eq: edgewise Poisson}, the normalization constant is supposed to be $\prod_{(i,j) \in \mathbb{Y}} \me^{-\exp(\theta)}$ instead of $\prod_{(i,j) \in \mathbb{Y}}\frac{1}{\exp(\theta)}$. 
However, these mentioned errors do not affect his conclusion on the respective models.

Besides the Poisson and Geometric Distribution, there are other \textit{reference measures} which corresponds to other \textit{baseline distribution}.
\citet{countergmapplied} introduced \textit{binomial-reference} and \textit{truncated geometric-reference} where it still focus on count edge's value, but there is a cap on the maximum attainable value.  
These constrained models will not  be explored in this paper.

\section{\textit{Poisson-Reference} ERGM Count Modelling}
Previously, the form of the ERGM Count was reduced to edge-wise Poisson and Geometric Distribution. 
These two models are rather simple and easily interpreted. 
However, both may not be the model that best fit the later application on Chapter 4.
In addition, these two modes do not take into account the complex relationship of a network.
We will explore more complex models and their interpretations later.

In this section, we will focus on \textit{poisson-reference} ERGM Count without complex constraints: $\mathcal{Y} \in \mathcal{N}_0^{\mathcal{Y}}$ and $h(\bm{y}) = \prod_{(i,j) \in \mathbb{Y}}(y_{ij}!)^{-1}$. 

Furthermore, the linear ERGM Count restriction will also be applied where $\eta(\bm{\theta}) = \bm{\theta}$ and thus $q = p$. 
This restriction makes the model comprehensible. 
In linear ERGM Count, if $\Theta_{N}$ is an open set, then $\theta_{k}^{\prime} >\theta_{k} \implies E(g_k(\bm{Y})|\theta,\theta_k^{\prime})>E(g_k(\bm{Y})|\theta,\theta_k)$. 
In other words, if other model parameters $\theta \setminus \theta_k$ are fixed, the expected sufficient statistics $g_k$ is strictly increasing in $\theta_k$.
Larger $\theta_k$ will result in a distribution of networks with more feature gauged by $g_k$.

\subsection{Exogenous Covariates}
Let the possibly predictive variables outside the network structure or exogenous covariate be $x_{ijk}$.
A \textit{poisson-reference} ERGM Count will reduce to Poisson Regression Model if we introduce an edge-independent sufficient statistics:
\begin{align}
g(\bm{y})= \sum_{(i,j) \in \mathbb{Y}} y_{ij}x_{ij}.
\end{align}

Let the model sufficient statistics only be exogenous covariate and \BI{sum}  the PMF will reduce to:
\begin{align*}
Pr(\bm{Y}=\bm{y}|\theta) &\propto \frac{\exp\left(\theta_1 \sum_{(i,j) \in \mathbb{Y}} y_{ij} + \theta_2 \sum_{(i,j) \in \mathbb{Y}} y_{ij}x_{ij} \right)}{\prod_{(i,j) \in \mathbb{Y}}y_{ij}!} \\
&= \prod_{(i,j) \in \mathbb{Y}} \frac{(\exp(\theta_1+\theta_2 x_{ij2}))^{y_{ij}}}{y_{ij}!}
\end{align*}
This can be recognised as an edge-wise distribution $Y_{ij}\stackrel{\text{iid}}{\sim}\text{poisson}(\lambda =\exp(\theta_1 + \theta_2 x_{ij}))$ .
Notice that adding exogenous covariate will maintain the poisson relationship yet changing the parameter $\lambda$ for edge $(i,j)$ by multiplication of factor $e^{\theta_2 x_{ij}}$.
It is obvious that further addition of another exogenous covariate will give the same effect.
Hence, this model is equivalent to applying Poisson regression model to the network data.  

In current R implementation of ERGM Count, there can be 3 representation of exogenous covariates for directed network. 
These are \BI{nodeocov} which is the attribute of the actor where the edge originate $x_{i}$ and \BI{nodeicov} which is the attribute of receiving actor $x_j$. 
The last one is $x_{ij}$ or \BI{edgecov} which represent the external edge attribute.

\subsection{Zero Modification}
The idea behind zero modification is to move away from the baseline distribution with new PMF that has more accurate representation of its $Pr(Y_{ij}=0)$. 
The proposed statistics to represent zero modification is \BI{nonzero}:
\begin{align}
g(\bm{y}) = \sum_{(i,j) \in \mathbb{Y}} \mathbb{I}(y_{ij} > 0)
\end{align}
This statistics is simply counting the number of edges that has value bigger than 0. 
Suppose we model ERGM Count that only contain \BI{sum} and \BI{nonzero} statistics. 
Then the PMF will reduce to:
\begin{align*}
Pr(\bm{Y}=\bm{y}|\theta_1,\theta_2) &\propto \frac{\exp\left(\theta_1 \sum_{(i,j) \in \mathbb{Y}} y_{ij} + \theta_2 \sum_{(i,j) \in \mathbb{Y}} \mathbb{I}(y_{ij} > 0)\right)}{\prod_{(i,j) \in \mathbb{Y}}y_{ij}!}\\
&=\prod_{(i,j) \in \mathbb{Y}} \frac{\exp \left(\theta_1 y_{ij} + \theta_2 \mathbb{I}(y_{ij} > 0)\right)}{y_{ij}!}
\end{align*}

This can be recognised as independent edge-wise Zero Modified Poisson, which is a Poisson Distribution with $Pr(Y_{ij}=0)$ can be adjusted by $\theta_2$ while $Pr(Y_{ij}=y_{ij}), y_{ij}=1,2,\ldots,$ maintain their relative proportion.
The corresponding parameter $\theta_2$ can take on any real values. 
If $\theta_2 <0$, then the probability of an edge to take value of zero is higher than Poisson Distribution. 
Whereas if $\theta_2>0$, the number of non-zero edge has higher probability than Poisson Distribution, thus giving the name \textit{\textbf{nonzero}}. Figure \ref{fig: Zero Modified Poisson} will illustrate this relatioship.

\begin{figure}[H]
<<fig.asp=0.65,eval=T>>=
zero_modified<- readRDS(paste(root,"/plots/zero_modified",sep=""))
plot(zero_modified)
@
\caption ["Illustration of Zero-Modified Poisson"]{This figure shows the effect of adding on \textit{\textbf{nonzero}} to the Poisson model. The top axis indicates parameter $\theta_2$, which affect the probability of zero edges. Notice that the effect diminished as parameter $\theta_1$ (right axis) gets higher. Furthermore, $e^{\theta_1}$ is the mean parameter when $\theta_2=0$.}
\label{fig: Zero Modified Poisson}
\end{figure}


\subsection{Dispersion Modelling}
Poisson distribution has only one parameter which control the mean and the spread of the distribution. 
The variance is constrained to be equal to the mean and is not adjustable.
One method to overcome this constrain is by adding sufficient statistic, \BI{CMP}:
\begin{align}
g(\bm{y}) = \sum_{(i,j) \in \mathbb{Y}} \log(y_{ij}!)
\end{align}
Suppose the sufficient statistics use in the model are \BI{sum} and \BI{CMP} with poisson reference, the PMF:
\begin{align*}
Pr(\bm{Y}=\bm{y}|\theta_1,\theta_2) &\propto \frac{\exp\left(\theta_1 \sum_{(i,j) \in \mathbb{Y}} y_{ij} + \theta_2 \sum_{(i,j) \in \mathbb{Y}} \log(y_{ij}!)\right)}{\prod_{(i,j) \in \mathbb{Y}}y_{ij}!}\\
&\propto \prod_{(i,j) \in \mathbb{Y}} (\exp(\theta_1))^{y_{ij}} (y_{ij}!)^{\theta_2-1}
\end{align*}
Hence the model reduces to the edge-wise \textit{Conway-Maxwell-Poisson} \textit{\textbf{CMP}} distribution $Y_{ij}\stackrel{\text{iid}}{\sim}\text{CMP}(\lambda =\exp(\theta_1),\nu = 1-\theta_2)$ where $\theta_2 \le 1$. 
When $\theta_2$ decreases from 0 to $-\infty$, the distribution become less and less dispersed compare to Poisson.
When $\theta_2 \to -\infty$, it is reduced to Bernoulli Distribution \citep{countergmdefined}. 
Whereas if as $0 < \theta_2 < 1$, the distribution become more dispersed as compared to Poisson. 
It reduces  to Geometric Distribution at $\theta_2=1$. 
Figure \ref{fig: CMP} will illustrate the various shapes of CMP distribution.

\begin{figure}[H]
<<fig.asp=.65>>=
cmp<- readRDS(paste(root,"/plots/cmp",sep=""))
plot(cmp)
@
\caption ["Illustration of Conway-Maxwell-Poisson Distribution"]{This figure shows the various shapes of Conway-Maxwell-Poisson Distribution (CMP). The right axis corresponds to parameter $\theta_1$. Whereas the top axis indicates parameter $\theta_2$, which dominates the dispersion effect of the distribution. When $\theta_2 = 0$, CMP reduce to Poisson with mean $e^{\theta_1}$. When $\theta_2 >0$ the distribution become more dispersed and it gets less dispersed when $\theta_2<0$.}  
\label{fig: CMP}
\end{figure}

\subsection{Mutuality}
The previous sections discussed the various models that have independent edge-wise assumption. 
Although, normalization constants tend to be intractable, they can be easily recognised as one particular distribution. 
However, fitting network data to independent edge-wise model will undermine the inherent relational characteristics of networks. 

In directed networks, the strength of mutuality is one of the network structure that one might consider. In friendship network, mutuality might be defined as when one person claimed to be friend with someone, the opposite party will reciprocate and claim the same. It can also be defined when one declare to be an enemy, the other party also declare the referral to be an enemy. Surely, the tendency to be mutual differs on different network. Hence being mutual in binary network can be defined as $y_{ij}=y_{ji}=1$ or sometimes $y_{ij}=y_{ji}$.

Addition of mutuality will make the ERGM Count model difficult to interpret. \citet{countergmdefined} suggest that we can understand the model through conditional distribution:
$Pr(Y_{ij}=y_{ij}| \bm{Y} \in \mathcal{Y}_{ij}(y))$ where 
$\mathcal{Y}_{ij}(y)$ is a set where only edge $y_{ij}$ can change it's value where others assumed to be constant.
This means that we will know which value $y_{ij} \in \mathbb{N}_0$ will random variable $Y_{ij}$ likely to attain, given one particular network values. 

However, we will proof that addition of mutuality will reduce to model with independent pair-wise distribution. 
Where a pair represents 2 edges where the origin and destinaton nodes are opposite of each other.
The proposed statistics to represent mutuality are:
\begin{itemize}

\item \textit{\textbf{Minimum}}

One of the proposed statistics to represent mutuality is minimum value or \textit{\textbf{mutual(min)}}: 
\begin{align}
g(\bm{y}) = \sum_{(i,j)\in\mathbb{Y},i<j} \min(y_{ij},y_{ji})
\end{align}
This is simply taking minimum values between 2 opposite edges of a dyad, and sum it for every possible pair.
When a model only use the \textit{\textbf{sum}} and \textit{\textbf{mutual(min)}}, the PMF:
\begin{align*}
Pr(\bm{Y}=\bm{y}|\theta_1,\theta_2) &\propto \frac{\exp\left(\theta_1 \sum_{(i,j) \in \mathbb{Y}} y_{ij} + \theta_{2}\sum_{(i,j)\in\mathbb{Y},i<j} \min(y_{ij},y_{ji})\right)}{\prod_{(i,j) \in \mathbb{Y}}y_{ij}!}\\
&= \frac{\exp\left(\theta_1 \sum_{(i,j) \in \mathbb{Y},i<j} (y_{ij}+y_{ji}) + \theta_{2}\sum_{(i,j)\in\mathbb{Y},i<j} \min(y_{ij},y_{ji})\right)}{\prod_{(i,j) \in \mathbb{Y},i<j}y_{ij}!y_{ji}!}\\
&= \prod_{(i,j) \in \mathbb{Y},i<j} \frac{\exp(\theta_1(y_{ij}+y_{ji})+\theta_2 \min(y_{ij},y_{ji}))}{y_{ij}!y_{ji}!}
\end{align*}
Note that this cant be reduced into edge-wise distribution as per previous sections. Yet it has indendent pair-wise distribution where $\forall(i,j) \in\mathbb{Y},i<j$, the joint distribution of $(Y_{ij},Y_{ji})$ has PMF:
\begin{align*}
P(Y_{ij}=y_{ij},Y_{ji}=y_{ji}|\theta_1,\theta_2) \propto \frac{\exp(\theta_1(y_{ij}+y_{ji})+\theta_2 \min(y_{ij},y_{ji}))}{y_{ij}!y_{ji}!}
\end{align*}

\item \textit{\textbf{Negative Absolute Difference}}

Another proposed statistics is negative absolute difference or \textit{\textbf{mutual(nabsdiff)}} which has the form:
\begin{align}
g(\bm{y}) = \sum_{(i,j)\in\mathbb{Y},i<j} -|y_{ij} - y_{ji}| 
\end{align}
Note that the statistics g's maximum value is 0. 
It is achieved when $\forall(i,j) \in\mathbb{Y},i<j (y_{ij}=y_{ji})$ or every pair is equal. 
The negation of the statistics is necessary as the higher the statistics, the higher strength of mutuality. 
With the same method as above, the pairwise distribution $(Y_{ij},Y_{ji})$ has PMF:
\begin{align*}
Pr(Y_{ij}=y_{ij},Y_{ji}=y_{ji}| \theta_1,\theta_2) \propto \frac{\exp(\theta_1 (y_{ij}+y_{ji}) - \theta_2 |y_{ij}-y_{ji}|)}{y_{ij}!y_{ji}!} 
\end{align*}

\item \textit{\textbf{Geometric Mean}}

The last proposed statistics is geometric mean or \textit{\textbf{mutual(geomean)}} which has the form:
\begin{align}
g(\bm{y}) = \sum_{(i,j)\in\mathbb{Y},i<j} \sqrt{y_{ij}}\sqrt{y_{ji}} 
\end{align}
Similarly the model with statistics \textit{\textbf{sum}} and \textit{\textbf{mutual(geomean)}} will have independent pairwise ditribution $(Y_{ij},Y_{ji})$ with PMF:
\begin{align*}
Pr(Y_{ij}=y_{ij},Y_{ji}=y_{ji}| \theta_1,\theta_2) \propto \frac{\exp(\theta_1 (y_{ij}+y_{ji}) + \theta_2 \sqrt{y_{ij}}\sqrt{y_{ji}})}{y_{ij}!y_{ji}!} 
\end{align*}
\end{itemize}
Before we discuss the differences of each parameter, it is best to look at the  shapes of the joint distribution. 
\begin{figure}[H]
<< fig.asp=0.7,>>=
mutual<- readRDS(paste(root,"/plots/mutual",sep=""))
plot(mutual)
@
\caption [Joint distribution with mutual statistics]{This image shows the various PMF of the joint distribution with $\theta_1 =1 $. The different shades of black indicate the region that has higher probability. Here the top axis represent $\theta_2$ which coressponds to the parameter of \textit{\textbf{mutual}} statistics. When $\theta_2 = 0$, the joint distibution follows 2 independent Poisson with $\lambda = e^{\theta_1}$}
\end{figure}

The differences of the joint distributions lies on the dependency statistics $\min(y_ij,y_ji),-|y_{ij}-y_{ji}|, \text{ and} \sqrt{y_{ij}}\sqrt{y_{ji}}$ for 
 \BI{min}, \BI{nabsdiff} and \BI{geomean} respectively.
When $\theta_2>0$ for \BI{min} and \BI{geomean}, the probability will increase as $y_{ij}$ and $y_{ji}$ increase. 
This shifted the distribution to higher pair values. 
For \BI{nabsdiff} the level of $y_{ij}$ and $y_{ji}$ does not affect the reduction of probability. 
However, the reduction occurs less when disparity between $y_{ij}$ and $y_{ji}$ is low and 0 reduction at $y_{ij}=y_{ji}$. 
Hence the distribution is "pulled-in" to the mutual values. 
The same occurs in \BI{min}, such that the highest increase occurs when $y_{ij}=y_{ji}$. 
Hence the distribution is "pulled-up" to mutual values.

When $\theta_2 <0$, \BI{min} and \BI{geomean} will experience higher reduction at higher values of $y_{ij}$ and $y_{ji}$. The reduction stop as $y_{ij}$ or $y_{ji}$ equals to zero. 
Hence, it creates bimodal distribution that has low values with opposing edge has zero value. 
The \BI{min} shows less abrupt clouds as the reduction varies. 
The effect differs significantly from \BI{nabsdiff}. 
There is increase in probability especially when the disparity is high, which creates the bimodal distribution. 
Since now the level of $y_{ij}$ or $y_{ji}$ affect the increase, the distribution cloud clutter on the higher values with opposinge edge is close to 0.

\subsection{Transitivity}
not done
